{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22491,"status":"ok","timestamp":1645404609023,"user":{"displayName":"Nazia Afreen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15576392179408150645"},"user_tz":-360},"id":"eqkTyvcVOSiP","outputId":"ae3aeaa4-43ee-4c08-ab64-236db06b5479"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4589,"status":"ok","timestamp":1645404613604,"user":{"displayName":"Nazia Afreen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15576392179408150645"},"user_tz":-360},"id":"wwb4tONrO6cl","outputId":"f4c9d2ad-f865-400c-9b54-3d45ca8e00b6"},"outputs":[],"source":["!pip install tensorflow_addons"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3925,"status":"ok","timestamp":1645404617525,"user":{"displayName":"Nazia Afreen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15576392179408150645"},"user_tz":-360},"id":"hepWxdaiLw2y"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","tfa.register.register_all()\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import numpy.random as rng\n","import seaborn as sns\n","import pickle\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":642,"status":"ok","timestamp":1645404618157,"user":{"displayName":"Nazia Afreen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15576392179408150645"},"user_tz":-360},"id":"WQvvp5Q5Lw23"},"outputs":[],"source":["class PokeMetric:\n","    def __init__(self, width, height, n_channel, support_dataset, support_size=151):\n","        self.width = width\n","        self.height = height\n","        self.n_channel = n_channel\n","        self.support = []\n","        self.model = None\n","        self.test_images = []\n","        self.test_labels = []\n","        self.current_epoch = 0\n","        self.support_size = support_size\n","        self.load_support_images(support_dataset)\n","\n","    def load_support_images(self, dataset_location):\n","        self.support = []\n","        labels = []\n","        for data in os.listdir(dataset_location):\n","            name = data.split('.')[0]\n","            loading_mode = 0 if self.n_channel == 1 else 1\n","            if name.isdigit() and int(name) <= self.support_size:\n","                image = cv2.imread(dataset_location + data, loading_mode)\n","                image = cv2.resize(image, (self.width, self.height))\n","                self.support.append(image / 255.)\n","                labels.append(int(name))\n","        self.support = [self.support[i] for i in np.argsort(labels)]\n","\n","    def load_val_images(self, dataset_location):\n","        self.test_images = []\n","        self.test_labels = []\n","        loading_mode = 0 if self.n_channel == 1 else 1\n","        for data in os.listdir(dataset_location):\n","            name = data.split('-')[0]\n","            if int(name) <= self.support_size:\n","                image = cv2.imread(dataset_location + data, loading_mode)\n","                image = cv2.resize(image, (self.width, self.height))\n","                self.test_images.append(image / 255.)\n","                self.test_labels.append(int(name)-1)\n","        self.test_labels = np.array(self.test_labels)\n","\n","    def build_model(self, drop_rate=0, kernel_reg=0, std_init=0.01):\n","        input_shape = (self.width, self.height, self.n_channel)\n","        left_input = tf.keras.layers.Input(input_shape)\n","        right_input = tf.keras.layers.Input(input_shape)\n","        \n","        w_init = tf.keras.initializers.RandomNormal(0,std_init)\n","        b_init = tf.keras.initializers.RandomNormal(0.5,std_init)\n","        regul = tf.keras.regularizers.l2(kernel_reg)\n","\n","        convnet = tf.keras.models.Sequential()\n","        convnet.add(tf.keras.layers.Conv2D(64, (10, 10), activation='relu', input_shape=input_shape,\n","                                          kernel_initializer=w_init, kernel_regularizer=regul))\n","        convnet.add(tf.keras.layers.MaxPooling2D())\n","        convnet.add(tf.keras.layers.Conv2D(128, (7, 7), activation='relu',\n","                                          kernel_initializer=w_init, kernel_regularizer=regul,bias_initializer=b_init))\n","        convnet.add(tf.keras.layers.MaxPooling2D())\n","        convnet.add(tf.keras.layers.Conv2D(128, (4, 4), activation='relu',\n","                                          kernel_initializer=w_init, kernel_regularizer=regul,bias_initializer=b_init))\n","        convnet.add(tf.keras.layers.MaxPooling2D())\n","        convnet.add(tf.keras.layers.Conv2D(256, (4, 4), activation='relu',\n","                                          kernel_initializer=w_init, kernel_regularizer=regul,bias_initializer=b_init))\n","        convnet.add(tf.keras.layers.Dropout(drop_rate))\n","        convnet.add(tf.keras.layers.Flatten())\n","        convnet.add(tf.keras.layers.Dense(4096, activation=\"sigmoid\",\n","                                         kernel_initializer=tf.keras.initializers.RandomNormal(0,0.1), \n","                                          kernel_regularizer=regul,\n","                                          bias_initializer=tf.keras.initializers.RandomNormal(0,0.1)))\n","        convnet.add(tf.keras.layers.Dropout(drop_rate))\n","\n","        encoded_l = convnet(left_input)\n","        encoded_r = convnet(right_input)\n","        merged = tf.math.abs(encoded_l - encoded_r)\n","        prediction = tf.keras.layers.Dense(1, activation='sigmoid',\n","                                          kernel_initializer=tf.keras.initializers.RandomNormal(0,0.1), \n","                                          kernel_regularizer=regul,\n","                                          bias_initializer=tf.keras.initializers.RandomNormal(0,0.1))(merged)\n","\n","        self.model = tf.keras.Model(inputs=[left_input, right_input], outputs=prediction)\n","        self.current_epoch = 0\n","\n","    def compile(self, optimizer, loss):\n","        self.model.compile(optimizer, loss)\n","\n","    def get_batch(self, batch_size=32):\n","        #get a batch of training pairs, positive get labeled 1 (same images) negative get labeled 0 (different images)\n","        pairs = [[], []]\n","        # data augmentation by randomly rotate\n","        train_support = np.array(tfa.image.rotate(np.array(self.support).reshape(self.support_size, self.width, self.height, self.n_channel),\n","                                         rng.randn(self.support_size)))\n","        # avoid overfitting by adding random noise to the background full of zeros\n","        train_support += (0.1*rng.randn(self.support_size, self.width, self.height, self.n_channel) + 0.5)*(train_support == 0)\n","        ref = rng.randint(0, self.support_size, batch_size//2)\n","        indices = list(np.arange(self.support_size))\n","        pairs[0] = [train_support[i] for i in ref]\n","        pairs[1] = [train_support[i] for i in ref]\n","\n","        \n","        for i in range(batch_size//2):\n","            pairs[0].append(train_support[ref[i]])\n","            indices.remove(ref[i])\n","            pairs[1].append(train_support[rng.choice(indices)])\n","            indices.append(ref[i])\n","        pairs[0] = np.array(pairs[0])\n","        pairs[1] = np.array(pairs[1])\n","        labels = np.array([1]*(batch_size//2) + [0]*(batch_size//2))\n","        del train_support\n","        return pairs, labels\n","    \n","    def get_val_batch(self, batch_size):\n","        # get a validation batch meaning negative and positive pairs with the first from the support dataset and the second from the ecg images\n","        pairs = [[], []]\n","        ref = rng.randint(0, self.support_size, batch_size//2)\n","        pairs[0] = [self.support[i] if i in self.test_labels else self.support[0] for i in ref]\n","        pairs[1] = [self.test_images[rng.choice(np.where(self.test_labels == i)[0])] if i in self.test_labels \n","                    else self.test_images[rng.choice(np.where(self.test_labels == 0)[0])] for i in ref]\n","        for i in range(batch_size//2):\n","            pairs[0].append(self.support[ref[i]])\n","            pairs[1].append(self.test_images[rng.choice(np.where(self.test_labels != i)[0])])\n","        pairs[0] = np.array(pairs[0])\n","        pairs[1] = np.array(pairs[1])\n","        labels = np.array([1]*(batch_size//2) + [0]*(batch_size//2))\n","        return pairs, labels\n","    \n","    def train(self, n_batch, batch_size, saving_period=100, val_step=100):\n","        val_loss_track = []\n","        train_loss_track = []\n","        for i in range(n_batch):\n","            x, y = self.get_batch(batch_size)\n","            train_loss = self.model.train_on_batch(x, y)\n","            del x\n","            del y\n","            if i % saving_period == 0 and i != 0:\n","                self.model.save(\"poke\" + str(self.current_epoch + i))\n","            if i % val_step == 0:\n","                print(i)            \n","                train_loss_track.append(train_loss)\n","                x_val, y_val = self.get_val_batch(batch_size)\n","                val_loss = self.model.test_on_batch(x_val, y_val)\n","                del x_val\n","                del y_val                \n","                val_loss_track.append(val_loss)\n","\n","\n","        self.current_epoch += n_batch\n","        self.model.save('./drive/MyDrive/kaggle-one-shot-pokemon/model')\n","\n","        return train_loss_track, val_loss_track\n","    \n","    def test(self, test_size):\n","        correct = 0\n","        results = []\n","        y_pred = []\n","        selected_indices = rng.randint(0, len(self.test_images), test_size)\n","        selected = ([self.test_images[i] for i in selected_indices], [self.test_labels[i] for i in selected_indices])\n","        x = np.array(selected[1])\n","        labels = np.unique(x)\n","        labels = list(labels)\n","        for i in range(test_size):\n","            r = self.prediction(selected[0][i], test=True)\n","            y_pred.append(r)\n","            if r == selected[1][i]:\n","                correct += 1\n","            results.append((r,selected[1][i]))\n","        #print(classification_report(selected[1], y_pred, labels=labels))\n","        return (correct/test_size)*100, results\n","\n","    def prediction(self, img, test=False):\n","        if not test:\n","            img = cv2.resize(img, (self.width, self.height))/255.\n","        pairs = [np.array([img]*self.support_size), np.array(self.support)]\n","        probs = self.model.predict(pairs)\n","        return np.argmax(probs)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":18268,"status":"ok","timestamp":1645404636423,"user":{"displayName":"Nazia Afreen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15576392179408150645"},"user_tz":-360},"id":"yty9O_pXLw2_"},"outputs":[],"source":["support_dataset = \"./drive/MyDrive/kaggle-one-shot-pokemon/pokemon-a/\"\n","test_dataset = \"./drive/MyDrive/kaggle-one-shot-pokemon/pokemon-tcg-images/\"\n","inputs = []\n","values = []\n","width = 128\n","height = 128\n","pokemetric = PokeMetric(width, height, 1, support_dataset, 50)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":125009,"status":"ok","timestamp":1645404761419,"user":{"displayName":"Nazia Afreen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15576392179408150645"},"user_tz":-360},"id":"7FBqVOy0Lw2_"},"outputs":[],"source":["pokemetric.load_val_images(test_dataset)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2310,"status":"ok","timestamp":1645404763707,"user":{"displayName":"Nazia Afreen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15576392179408150645"},"user_tz":-360},"id":"WvHtoXTOLw3A"},"outputs":[],"source":["pokemetric.build_model(drop_rate=0.3, kernel_reg=0.0002)\n","pokemetric.compile(tf.keras.optimizers.Adam(learning_rate=0.00006), tf.losses.BinaryCrossentropy())\n","t,v = [], []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"x6-Jdg8HLw3B"},"outputs":[],"source":["tbis, vbis = pokemetric.train(50001, 32, saving_period=5000)\n","t += tbis\n","v += vbis\n","\n","plt.figure(figsize = (15,10))\n","plt.plot(t)\n","\n","plt.title(\"Losses\")\n","plt.legend(['training'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUe5fDjxzFMV"},"outputs":[],"source":["r = pokemetric.test(20)\n","print(r[0])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"project pokemon.ipynb","provenance":[{"file_id":"1hT9Ieeci4jKi_aB9ZTqq8KYbzZOfEAwF","timestamp":1645398859315}],"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}
